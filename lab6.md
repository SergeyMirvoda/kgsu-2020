# Лабораторная 6. Интеграция с HDFS

> Цель: в аналитической системе получить данные из централизованного хранилища.
> Данные будут храниться в HDFS, доступ к ним организовать из ClickHouse.  

## Задание

1. Ознакомиться с документацией  
1.1 движок ``HDFS`` https://clickhouse.tech/docs/ru/engines/table-engines/integrations/hdfs/  
1.2 табличная функция ``hdfs``https://clickhouse.tech/docs/ru/sql-reference/table-functions/hdfs/  

2. Подключитесь к тестовой виртуальной машине по ssh
3. Запустите ``clickhouse-client``  
3.1 Выполните команду  
```
 show create table lab6.big_table
```
Результатом вызова является скрипт создания таблицы. Обратите внимание на настройки движка ``HDFS``  
В какой папке хранятся данные? Данные какого формата лежат в папке? Что означает ``{0..9}`` в имени файла?  
3.2 Выполните команду
```
 select * from lab6.big_table order by value  
```
Запомните значения, далее сравним их с теми, что хранятся в ``hadoop``  
3.3 Пролистайте папку в котрой лежат данные таблицы (адрес папки в настройках движка см. 3.1)  

```
 hdfs dfs -ls hdfs://{сюда_вставить_адрес}/{сюда_вставить_имя_папки_из_настроек_движка}  
```
Сколько файлов находится в папке?  
3.4 С помощью команды ``-cat`` прочтитайте содержимое файлов и сравните с полученным в 3.2  
4. Добавление данных  
4.1 Вспомните свойства формата ``CSV`` https://clickhouse.tech/docs/en/interfaces/formats/#csv  
4.2 Создайте простой текстовый файл с данными в формате ``CSV``, например такой:    
```
"ten",10
"eleven",11
```
и сохраните его под именем ``file{следующее_незанятое_число_в_папке_hdfs}``  
4.3 С помощью переменной  окружения ``HADOOP_USER_NAME`` задайте имя пользователя из под котрого будете обращаться к ``hadoop``:  
```
export HADOOP_USER_NAME=hadoop
```
4.4 С помощью команды ``-put`` запишите этот файл в папку с котрой свзязана таблица ``big_table``  
4.5 Выполните запрос 3.2. Появились ваши данные в результатах?  


